{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "porto-seguro.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPSX4uV0nra07coeEWDi1FX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tom-Lennox/Kaggle/blob/master/porto_seguro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WttrAqfbAmEx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 【行ったこと】\n",
        "\n",
        "# LogisticRegression、特に設定しなければrandomと同じくらいのscore.\n",
        "# auc:0.5がランダムと同一。\n",
        "# ▼ \n",
        "\n",
        "# ▼ 3 4の準備\n",
        "\n",
        "# ▼ 4 hyper param　の調整\n",
        "\n",
        "\n",
        "# 【memo】\n",
        "# future_engneering > paramの調整\n",
        "# "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZgkrSeyAn4g",
        "colab_type": "code",
        "outputId": "6a6a7a2f-508c-4a49-eaa9-c758a1878ff2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        }
      },
      "source": [
        "# kaggle APIセット\n",
        "!pip install kaggle\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "import io, os\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "drive_service = build('drive', 'v3')\n",
        "results = drive_service.files().list(\n",
        "        q=\"name = 'kaggle.json'\", fields=\"files(id)\").execute()\n",
        "kaggle_api_key = results.get('files', [])\n",
        "\n",
        "filename = \"/content/.kaggle/kaggle.json\"\n",
        "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "\n",
        "request = drive_service.files().get_media(fileId=kaggle_api_key[0]['id'])\n",
        "fh = io.FileIO(filename, 'wb')\n",
        "downloader = MediaIoBaseDownload(fh, request)\n",
        "done = False\n",
        "while done is False:\n",
        "    status, done = downloader.next_chunk()\n",
        "    print(\"Download %d%%.\" % int(status.progress() * 100))\n",
        "os.chmod(filename, 600)\n",
        "# 「Download 100%.」と表示で成功。\n",
        "\n",
        "# [kaggle.json]を持参する。\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# jsonファイルを指定の場所に配置\n",
        "!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json\n",
        "# titanicをダウンロードします。\n",
        "!kaggle competitions download -c porto-seguro-safe-driver-prediction\n",
        "\n",
        "# jsonファイルを指定の場所に配置\n",
        "!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json\n",
        "# titanicをダウンロードします。\n",
        "!kaggle competitions download -c porto-seguro-safe-driver-prediction\n",
        "!ls\n",
        "\n",
        "# ▼ unzip\n",
        "!unzip '*.zip'\n",
        "!ls\n",
        "# ▲ unzip\n",
        "\n",
        "# ▼ kaggle notebookの場合\n",
        "# # !cd ../\n",
        "# !ls ../input/porto-seguro-safe-driver-prediction\n",
        "\n",
        "# TRAIN_DATA = '../input/porto-seguro-safe-driver-predicti1on/train.csv'\n",
        "# df = pd.read_csv(TRAIN_DATA)\n",
        "# ▲ kaggle notebookの場合\n",
        "\n",
        "# ▼ 行表示\n",
        "# ctrl + m + l\n",
        "# ▲ 行表示"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.6)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.12.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2020.4.5.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Download 100%.\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "cp: cannot create regular file '/root/.kaggle/kaggle.json': No such file or directory\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 5, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/kaggle/__init__.py\", line 23, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/kaggle/api/kaggle_api_extended.py\", line 146, in authenticate\n",
            "    self.config_file, self.config_dir))\n",
            "IOError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/2.12M [00:00<?, ?B/s]\n",
            "100% 2.12M/2.12M [00:00<00:00, 70.8MB/s]\n",
            "Downloading test.csv.zip to /content\n",
            " 74% 33.0M/44.4M [00:00<00:00, 37.8MB/s]\n",
            "100% 44.4M/44.4M [00:00<00:00, 90.4MB/s]\n",
            "Downloading train.csv.zip to /content\n",
            " 30% 9.00M/30.0M [00:00<00:00, 37.4MB/s]\n",
            "100% 30.0M/30.0M [00:00<00:00, 86.2MB/s]\n",
            "adc.json  sample_data\t\t     test.csv.zip\n",
            "drive\t  sample_submission.csv.zip  train.csv.zip\n",
            "Archive:  test.csv.zip\n",
            "  inflating: test.csv                \n",
            "\n",
            "Archive:  sample_submission.csv.zip\n",
            "  inflating: sample_submission.csv   \n",
            "\n",
            "Archive:  train.csv.zip\n",
            "  inflating: train.csv               \n",
            "\n",
            "3 archives were successfully processed.\n",
            "adc.json     sample_submission.csv\ttest.csv.zip\n",
            "drive\t     sample_submission.csv.zip\ttrain.csv\n",
            "sample_data  test.csv\t\t\ttrain.csv.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yJQWWypBdF9",
        "colab_type": "code",
        "outputId": "a14efa8e-0736-4515-a0b0-6f032a425d33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from logging import getLogger\n",
        "\n",
        "TRAIN_DATA = './train.csv'\n",
        "TEST_DATA =  './test.csv'\n",
        "\n",
        "logger = getLogger(__name__)\n",
        "\n",
        "def read_csv(path):\n",
        "  logger.debug('enter')\n",
        "  logger.debug('exit')\n",
        "  return df\n",
        "\n",
        "def load_train_data():\n",
        "  logger.debug('enter')\n",
        "  df = pd.read_csv(TRAIN_DATA)\n",
        "  logger.debug('exit')\n",
        "  return df\n",
        "\n",
        "def load_test_data():\n",
        "  logger.debug('enter')\n",
        "  df = pd.read_csv(TEST_DATA)\n",
        "  logger.debug('exit')\n",
        "  return df\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  print(load_train_data().head())\n",
        "  print(load_test_data().head())\n",
        "  # print(load_train_data().info())\n",
        "  # print(load_test_data().info())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   id  target  ps_ind_01  ...  ps_calc_18_bin  ps_calc_19_bin  ps_calc_20_bin\n",
            "0   7       0          2  ...               0               0               1\n",
            "1   9       0          1  ...               0               1               0\n",
            "2  13       0          5  ...               0               1               0\n",
            "3  16       0          0  ...               0               0               0\n",
            "4  17       0          0  ...               1               1               0\n",
            "\n",
            "[5 rows x 59 columns]\n",
            "   id  ps_ind_01  ps_ind_02_cat  ...  ps_calc_18_bin  ps_calc_19_bin  ps_calc_20_bin\n",
            "0   0          0              1  ...               0               0               1\n",
            "1   1          4              2  ...               1               0               1\n",
            "2   2          5              1  ...               0               0               0\n",
            "3   3          0              1  ...               0               0               0\n",
            "4   4          5              1  ...               0               0               1\n",
            "\n",
            "[5 rows x 58 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l63IvMnDxRbY",
        "colab_type": "code",
        "outputId": "2f6aeb35-71ae-4210-d12b-9556b1cfa5ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# ▼ 1_簡単なsubmit\n",
        "from logging import StreamHandler, DEBUG, Formatter, FileHandler, getLogger\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "!mkdir result_tmp\n",
        "# from load_data import load_train_data\n",
        "\n",
        "DIR = 'result_tmp/'\n",
        "SAMPLE_SUBMIT_FILE = './sample_submission.csv'\n",
        "\n",
        "# 結果出力先\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  log_fmt = Formatter('%(asctime)s %(name)s %(lineno)d [%(levelname)s][%(funcName)s] %(message)s ')\n",
        "  handler = StreamHandler()\n",
        "  handler.setLevel('INFO')\n",
        "  handler.setFormatter(log_fmt)\n",
        "  logger.addHandler(handler)\n",
        "  # logの出力は任意、StreamHandler()を流す\n",
        "  # 標準出力：INFO　Level 以上\n",
        "  # ファイル出力：debug level 以上\n",
        "\n",
        "  handler = FileHandler(DIR + 'train.py.log', 'a')\n",
        "  handler.setLevel(DEBUG)\n",
        "  handler.setFormatter(log_fmt)\n",
        "  logger.setLevel(DEBUG)\n",
        "  logger.addHandler(handler)\n",
        "\n",
        "  logger.info('start')\n",
        "  #logを取るメリット：セクション間の実行時間がわかる。」\n",
        "  df = load_train_data()\n",
        "\n",
        "  x_train = df.drop('target', axis=1)\n",
        "  # targetはdataに入ると過学習になる。\n",
        "  y_train = df['target'].values\n",
        "  # valuesにするとpdからnp配列になる。\n",
        "  use_cols = x_train.columns.values\n",
        "  # 並び順固定\n",
        "  logger.debug('train columns: {} {}'.format(use_cols.shape, use_cols))\n",
        "  logger.info('data preparation end {}'.format(x_train.shape))\n",
        "\n",
        "  clf = LogisticRegression(random_state=0)\n",
        "  clf.fit(x_train, y_train)\n",
        "\n",
        "  logger.info('train end')\n",
        "\n",
        "  df = load_test_data()\n",
        "\n",
        "  x_test = df[use_cols].sort_values('id')\n",
        "  # + 念の為sort\n",
        "  logger.info('test data load end {}'.format(x_test.shape))\n",
        "  pred_test = clf.predict_proba(x_test)\n",
        "\n",
        "  df_submit = pd.read_csv(SAMPLE_SUBMIT_FILE).sort_values('id')\n",
        "  df_submit['target'] = pred_test\n",
        "\n",
        "  df_submit.to_csv(DIR + 'submit.csv')\n",
        "# ▲ 1_簡単なsubmit"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-17 22:43:07,612 __main__ 27 [INFO][<module>] start \n",
            "2020-05-17 22:43:09,934 __main__ 38 [INFO][<module>] data preparation end (595212, 58) \n",
            "2020-05-17 22:43:16,626 __main__ 43 [INFO][<module>] train end \n",
            "2020-05-17 22:43:20,250 __main__ 49 [INFO][<module>] test data load end (892816, 58) \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkYr-sf26VIm",
        "colab_type": "code",
        "outputId": "fb5ce4d5-2a6b-4b92-ec68-3255ccc0b67d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# ▼ 1_簡単なsubmit\n",
        "from logging import StreamHandler, DEBUG, Formatter, FileHandler, getLogger\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "  # ▼ 2_cross_validation追加\n",
        "from sklearn.model_selection import StratifiedKFold, ParameterGrid\n",
        "# 各cvの設定で01の引き率を一定にできる。※バスケット分析等は例外。\n",
        "from sklearn.metrics import log_loss, roc_auc_score\n",
        "  # ▲ 2_cross_validation追加\n",
        "!mkdir result_tmp\n",
        "# from load_data import load_train_data\n",
        "\n",
        "DIR = 'result_tmp/'\n",
        "SAMPLE_SUBMIT_FILE = './sample_submission.csv'\n",
        "\n",
        "# 結果出力先\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    log_fmt = Formatter('%(asctime)s %(name)s %(lineno)d [%(levelname)s][%(funcName)s] %(message)s ')\n",
        "    handler = StreamHandler()\n",
        "    handler.setLevel('INFO')\n",
        "    handler.setFormatter(log_fmt)\n",
        "    logger.addHandler(handler)\n",
        "    # logの出力は任意、StreamHandler()を流す\n",
        "    # 標準出力：INFO　Level 以上\n",
        "    # ファイル出力：debug level 以上\n",
        "\n",
        "    handler = FileHandler(DIR + 'train.py.log', 'a')\n",
        "    handler.setLevel(DEBUG)\n",
        "    handler.setFormatter(log_fmt)\n",
        "    logger.setLevel(DEBUG)\n",
        "    logger.addHandler(handler)\n",
        "\n",
        "    logger.info('start')\n",
        "    #logを取るメリット：セクション間の実行時間がわかる。」\n",
        "    df = load_train_data()\n",
        "\n",
        "    x_train = df.drop('target', axis=1)\n",
        "    # targetはdataに入ると過学習になる。\n",
        "    y_train = df['target'].values\n",
        "    # valuesにするとpdからnp配列になる。\n",
        "    use_cols = x_train.columns.values\n",
        "    # 並び順固定\n",
        "    logger.debug('train columns: {} {}'.format(use_cols.shape, use_cols))\n",
        "    logger.info('data preparation end {}'.format(x_train.shape))\n",
        "\n",
        "    # ▼ 2_cross_validation追加\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
        "    # StratifiedKFold：行のindexが返ってくる。\n",
        "      # ▼ 3\n",
        "    list_auc_score = []\n",
        "    list_logloss_score = []\n",
        "      # ▲ 3\n",
        "    for train_idx, valid_idx in cv.split(x_train, y_train):\n",
        "        # 01の比率はtaegetの方にしか寄らないので、xは何でも良い。\n",
        "        trn_x = x_train.iloc[train_idx, :]\n",
        "        val_x = x_train.iloc[valid_idx, :]\n",
        "        # pandasなので.iloc\n",
        "        # 列も(全部)指定。\n",
        "\n",
        "        # trn_y = x_train[train_idx]\n",
        "        # val_y = x_train[valid_idx]\n",
        "        # miss\n",
        "        trn_y = y_train[train_idx]\n",
        "        val_y = y_train[valid_idx]\n",
        "        # train ⇒ validation　計って評価する、を全部のsplitで行う。\n",
        "        # 手元cv_scoreとreader_boadのcvが一致するように適正にcloss_validationを切る。\n",
        "        # 一致しない場合はどこかに偏りが生じている。\n",
        "        clf = LogisticRegression(random_state=0)\n",
        "        clf.fit(x_train, y_train)\n",
        "\n",
        "        pred = clf.predict_proba(val_x)[:, 1]\n",
        "        sc_logloss = log_loss(val_y, pred)\n",
        "        sc_auc = roc_auc_score(val_y, pred)\n",
        "\n",
        "        # ▼ 3\n",
        "        list_logloss_score.append(sc_logloss)\n",
        "        list_auc_score.append(sc_auc)\n",
        "        logger.debug('  logloss: {}, auc: {}'.format(sc_logloss, sc_auc))\n",
        "        # log見てわかるようにindent下げる。\n",
        "        # ▲ 3\n",
        "        # ▼ 3\n",
        "        # logger.info('logloss: {}, auc: {}'.format(sc_logloss, sc_auc))\n",
        "        logger.info('logloss: {}, auc: {}'.format(np.mean(sc_logloss), np.mean(list_auc_score)))\n",
        "        # infoとdebug、何が違うのか？\n",
        "        # ▲ 3\n",
        "        # logloss\n",
        "        # auc\n",
        "    # ▲ 2_cross_validation追加\n",
        "\n",
        "    clf = LogisticRegression(random_state=0)\n",
        "    clf.fit(trn_x, trn_y)\n",
        "\n",
        "    logger.info('train end')\n",
        "\n",
        "    df = load_test_data()\n",
        "\n",
        "    x_test = df[use_cols].sort_values('id')\n",
        "    # + 念の為sort\n",
        "    logger.info('test data load end {}'.format(x_test.shape))\n",
        "    # ▼ 2_cross_validation追加\n",
        "    pred_test = clf.predict_proba(x_test)\n",
        "    # pred_test = clf.predict_proba(x_test)[:, 1]\n",
        "    # ▲ 2_cross_validation追加\n",
        "    df_submit = pd.read_csv(SAMPLE_SUBMIT_FILE).sort_values('id')\n",
        "    df_submit['target'] = pred_test\n",
        "\n",
        "    df_submit.to_csv(DIR + 'submit.csv')\n",
        "# ▲ 1_簡単なsubmit\n",
        "# # ▼ log\n",
        "# !ls ./result_tmp\n",
        "# !cat ./result_tmp/train.py.log\n",
        "# # ▲ log"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘result_tmp’: File exists\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-05-23 10:30:55,199 __main__ 33 [INFO][<module>] start \n",
            "2020-05-23 10:30:55,199 __main__ 33 [INFO][<module>] start \n",
            "2020-05-23 10:30:57,716 __main__ 44 [INFO][<module>] data preparation end (595212, 58) \n",
            "2020-05-23 10:30:57,716 __main__ 44 [INFO][<module>] data preparation end (595212, 58) \n",
            "2020-05-23 10:31:04,989 __main__ 83 [INFO][<module>] logloss: 0.18807471172878062, auc: 0.494807348616394 \n",
            "2020-05-23 10:31:04,989 __main__ 83 [INFO][<module>] logloss: 0.18807471172878062, auc: 0.494807348616394 \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cj7xxSzZRafm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "e11c98c2-6b46-40b1-b58c-fc1cb073bc48"
      },
      "source": [
        "# ▼ 1_簡単なsubmit\n",
        "from logging import StreamHandler, DEBUG, Formatter, FileHandler, getLogger\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "  # ▼ 2_cross_validation追加\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "# 各cvの設定で01の引き率を一定にできる。※バスケット分析等は例外。\n",
        "from sklearn.metrics import log_loss, roc_auc_score\n",
        "  # ▲ 2_cross_validation追加\n",
        "!mkdir result_tmp\n",
        "# from load_data import load_train_data\n",
        "\n",
        "DIR = 'result_tmp/'\n",
        "SAMPLE_SUBMIT_FILE = './sample_submission.csv'\n",
        "\n",
        "# 結果出力先\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  log_fmt = Formatter('%(asctime)s %(name)s %(lineno)d [%(levelname)s][%(funcName)s] %(message)s ')\n",
        "  handler = StreamHandler()\n",
        "  handler.setLevel('INFO')\n",
        "  handler.setFormatter(log_fmt)\n",
        "  logger.addHandler(handler)\n",
        "  # logの出力は任意、StreamHandler()を流す\n",
        "  # 標準出力：INFO　Level 以上\n",
        "  # ファイル出力：debug level 以上\n",
        "\n",
        "  handler = FileHandler(DIR + 'train.py.log', 'a')\n",
        "  handler.setLevel(DEBUG)\n",
        "  handler.setFormatter(log_fmt)\n",
        "  logger.setLevel(DEBUG)\n",
        "  logger.addHandler(handler)\n",
        "\n",
        "  logger.info('start')\n",
        "  #logを取るメリット：セクション間の実行時間がわかる。」\n",
        "  df = load_train_data()\n",
        "\n",
        "  x_train = df.drop('target', axis=1)\n",
        "  # targetはdataに入ると過学習になる。\n",
        "  y_train = df['target'].values\n",
        "  # valuesにするとpdからnp配列になる。\n",
        "  use_cols = x_train.columns.values\n",
        "  # 並び順固定\n",
        "  logger.debug('train columns: {} {}'.format(use_cols.shape, use_cols))\n",
        "  logger.info('data preparation end {}'.format(x_train.shape))\n",
        "\n",
        "  # ▼ 2_cross_validation追加\n",
        "  cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
        "  # StratifiedKFold：行のindexが返ってくる。\n",
        "      # ▼ 4 param： grid_search、総当たり。\n",
        "  all_params = {}\n",
        "  # dictionary作成\n",
        "  all_params = {'C': [10**i for i in range(-1, 2)],\n",
        "                'fit_intercept': [True, False],\n",
        "                'penalty': ['l2', 'l1'],\n",
        "                'random_state': [0]\n",
        "                }\n",
        "  min_score = 100\n",
        "  min_params = None\n",
        "  for params in ParameterGrid(all_params):\n",
        "      # ▲ 4 param： grid_search、総当たり。\n",
        "      # ▼ 4 param： grid_search、総当たり。\n",
        "        # ▼ 3\n",
        "      list_auc_score = []\n",
        "      list_logloss_score = []\n",
        "        # ▲ 3\n",
        "      for train_idx, valid_idx in cv.split(x_train, y_train):\n",
        "          # 01の比率はtaegetの方にしか寄らないので、xは何でも良い。\n",
        "          trn_x = x_train.iloc[train_idx, :]\n",
        "          val_x = x_train.iloc[valid_idx, :]\n",
        "          # pandasなので.iloc\n",
        "          # 列も(全部)指定。\n",
        "\n",
        "          # trn_y = x_train[train_idx]\n",
        "          # val_y = x_train[valid_idx]\n",
        "          # miss\n",
        "          trn_y = y_train[train_idx]\n",
        "          val_y = y_train[valid_idx]\n",
        "          # train ⇒ validation　計って評価する、を全部のsplitで行う。\n",
        "          # 手元cv_scoreとreader_boadのcvが一致するように適正にcloss_validationを切る。\n",
        "          # 一致しない場合はどこかに偏りが生じている。\n",
        "          \n",
        "        # ▲ 4 param： grid_search、総当たり。\n",
        "          clf = LogisticRegression(**params)\n",
        "          # **\n",
        "          clf.fit(x_train, y_train)\n",
        "\n",
        "          pred = clf.predict_proba(val_x)[:, 1]\n",
        "          sc_logloss = log_loss(val_y, pred)\n",
        "          sc_auc = roc_auc_score(val_y, pred)\n",
        "\n",
        "          list_logloss_score.append(sc_logloss)\n",
        "          list_auc_score.append(sc_auc)\n",
        "          logger.debug('  logloss: {}, auc: {}'.format(sc_logloss, sc_auc))\n",
        "        sc_logloss = np.mean(list_auc_score)\n",
        "        sc_auc = - np.mean(list_auc_score)      \n",
        "        logger.info('logloss: {}, auc: {}'.format(np.mean(sc_logloss, sc_auc))\n",
        "        if min_score > sc_auc:\n",
        "            min_score = sc_auc\n",
        "            min_param = params\n",
        "      logger.info('minimum params: {}'.format(min_params))\n",
        "      logger.info('minimum auc: {}'.format(min_score))\n",
        "  clf = LogisticRegression(**min_params)\n",
        "  clf.fit(trn_x, trn_y)\n",
        "\n",
        "  logger.info('train end')\n",
        "\n",
        "  df = load_test_data()\n",
        "\n",
        "  x_test = df[use_cols].sort_values('id')\n",
        "  # + 念の為sort\n",
        "  logger.info('test data load end {}'.format(x_test.shape))\n",
        "  # ▼ 2_cross_validation追加\n",
        "  pred_test = clf.predict_proba(x_test)\n",
        "  # pred_test = clf.predict_proba(x_test)[:, 1]\n",
        "  # ▲ 2_cross_validation追加\n",
        "  df_submit = pd.read_csv(SAMPLE_SUBMIT_FILE).sort_values('id')\n",
        "  df_submit['target'] = pred_test\n",
        "\n",
        "  df_submit.to_csv(DIR + 'submit.csv')\n",
        "# ▲ 1_簡単なsubmit\n",
        "# # ▼ log\n",
        "# !ls ./result_tmp\n",
        "# !cat ./result_tmp/train.py.log\n",
        "# # ▲ log"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-efa833a3da16>\"\u001b[0;36m, line \u001b[0;32m97\u001b[0m\n\u001b[0;31m    if min_score > sc_auc:\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-wptHVXJTiQ",
        "colab_type": "code",
        "outputId": "f6a2b045-0059-4f56-9c5d-bccfc4dedeeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "!ls ./result_tmp\n",
        "!cat ./result_tmp/train.py.log"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train.py.log\n",
            "2020-05-18 23:57:06,961 __main__ 32 [INFO][<module>] start \n",
            "2020-05-18 23:57:06,962 __main__ 17 [DEBUG][load_train_data] enter \n",
            "2020-05-18 23:57:09,214 __main__ 19 [DEBUG][load_train_data] exit \n",
            "2020-05-18 23:57:09,277 __main__ 42 [DEBUG][<module>] train columns: (58,) ['id' 'ps_ind_01' 'ps_ind_02_cat' 'ps_ind_03' 'ps_ind_04_cat'\n",
            " 'ps_ind_05_cat' 'ps_ind_06_bin' 'ps_ind_07_bin' 'ps_ind_08_bin'\n",
            " 'ps_ind_09_bin' 'ps_ind_10_bin' 'ps_ind_11_bin' 'ps_ind_12_bin'\n",
            " 'ps_ind_13_bin' 'ps_ind_14' 'ps_ind_15' 'ps_ind_16_bin' 'ps_ind_17_bin'\n",
            " 'ps_ind_18_bin' 'ps_reg_01' 'ps_reg_02' 'ps_reg_03' 'ps_car_01_cat'\n",
            " 'ps_car_02_cat' 'ps_car_03_cat' 'ps_car_04_cat' 'ps_car_05_cat'\n",
            " 'ps_car_06_cat' 'ps_car_07_cat' 'ps_car_08_cat' 'ps_car_09_cat'\n",
            " 'ps_car_10_cat' 'ps_car_11_cat' 'ps_car_11' 'ps_car_12' 'ps_car_13'\n",
            " 'ps_car_14' 'ps_car_15' 'ps_calc_01' 'ps_calc_02' 'ps_calc_03'\n",
            " 'ps_calc_04' 'ps_calc_05' 'ps_calc_06' 'ps_calc_07' 'ps_calc_08'\n",
            " 'ps_calc_09' 'ps_calc_10' 'ps_calc_11' 'ps_calc_12' 'ps_calc_13'\n",
            " 'ps_calc_14' 'ps_calc_15_bin' 'ps_calc_16_bin' 'ps_calc_17_bin'\n",
            " 'ps_calc_18_bin' 'ps_calc_19_bin' 'ps_calc_20_bin'] \n",
            "2020-05-18 23:57:09,277 __main__ 43 [INFO][<module>] data preparation end (595212, 58) \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}